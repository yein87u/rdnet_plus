import torch, timm
import torch.nn as nn
from timm.layers.squeeze_excite import EffectiveSEModule
from collections import OrderedDict
import torch.nn.functional as F

class RDNet_Tiny(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone = timm.create_model('rdnet_tiny.nv_in1k', pretrained=True, num_classes=0)
        self.fc = nn.Linear(1040, num_classes)

    def forward(self, x):
        embedings = self.backbone(x)
        out = self.fc(embedings)
        return embedings, out


class RDNet_Small(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone = timm.create_model('rdnet_small.nv_in1k', pretrained=True, num_classes=0)
        self.fc = nn.Linear(1264, num_classes)

    def forward(self, x):
        embedings = self.backbone(x)
        out = self.fc(embedings)
        return embedings, out


class RDNet_Base(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone = timm.create_model('rdnet_base.nv_in1k', pretrained=True, num_classes=0)
        self.fc = nn.Linear(1760, num_classes)
        # print(self.backbone)

    def forward(self, x):
        embedings = self.backbone(x)
        out = self.fc(embedings)
        return embedings, out


class RDNet_Large(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone = timm.create_model('rdnet_large.nv_in1k', pretrained=True, num_classes=0)
        self.fc = nn.Linear(2000, num_classes)

    def forward(self, x):
        embedings = self.backbone(x)
        out = self.fc(embedings)
        return embedings, out
    



# SpatialAttentionModule
class SpatialAttentionModule(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)
        self.sigmoid = nn.Sigmoid()
        self.latest_input_feature = None # ç”¨æ–¼å„²å­˜æœ€å¾Œçš„è¼¸å…¥ç‰¹å¾µ (B, C, H, W)
        self.latest_attn_map = None  # ç”¨æ–¼å„²å­˜æœ€å¾Œçš„æ³¨æ„åŠ›åœ– (B, 1, H, W)

    def forward(self, x):
        self.latest_input_feature = x.detach() # ç²å–è¼¸å…¥ç‰¹å¾µ
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_out = torch.cat([avg_out, max_out], dim=1)
        attn = self.sigmoid(self.conv1(x_out))
        self.latest_attn_map = attn.detach() # æ³¨æ„åŠ›åœ–

        return x * attn


# --- ä¸»æ¨¡å‹åŒ…è£ (å·²ä¿®æ”¹ç‚ºçµæ§‹æ›¿æ›) ---
class RDNet_Base_SAttention(nn.Module):
    """
    åŒ…è£å™¨é¡ï¼š
    - è¼‰å…¥ rdnet_base.nv_in1k(timm é è¨“ç·´æ¨¡å‹ï¼‰
    - åœ¨æ¯å€‹ EffectiveSEModule å¾Œæ’å…¥ SpatialAttentionModule (ä½¿ç”¨ nn.Sequential æ›¿æ›)
    """

    def __init__(self, num_classes: int, sa_kernel_size: int = 7, drop_rate=0.2):
        super().__init__()

        # è¼‰å…¥é è¨“ç·´æ¨¡å‹
        self.model = timm.create_model("rdnet_base.nv_in1k", pretrained=True, num_classes=0, drop_path_rate=drop_rate)
        # self.fc = nn.Linear(1760, num_classes)
        self.fc = nn.Sequential(
            nn.Dropout(p=0.5),  # æ–°å¢ Dropout
            NormedLinear(1760, num_classes)
        )
        # åŸ·è¡Œæ³¨å…¥
        self._inject_spatial_attention(sa_kernel_size)

    # -------------------------------------------------
    def _inject_spatial_attention(self, kernel_size):
        """
        æ‰¾å‡ºæ‰€æœ‰ EffectiveSEModule, ä¸¦ä½¿ç”¨ nn.Sequential æ›¿æ›
        ä»¥åŒ…å«åŸ ESE å’Œæ–°çš„ SA æ¨¡çµ„ã€‚
        """
        sa_index = 0
        
        # ä½¿ç”¨ list() è¤‡è£½ï¼Œå› ç‚ºæˆ‘å€‘æœƒåœ¨è¿­ä»£æ™‚ä¿®æ”¹ self.model çµæ§‹
        for name, module in list(self.model.named_modules()):
            if isinstance(module, EffectiveSEModule):
                
                # å‰µå»ºæ–°çš„ SA å¯¦ä¾‹
                sa_instance = SpatialAttentionModule(kernel_size)
                
                # å®šä½çˆ¶æ¨¡çµ„å’Œå­æ¨¡çµ„åç¨±
                # name = 'stages.0.blocks.0.ese' (timm model çš„å‘½åæ–¹å¼)
                parts = name.rsplit('.', 1)

                if len(parts) == 2:
                    parent_path, child_name = parts
                else:
                    # ESEModule æ˜¯é ‚å±¤æ¨¡çµ„
                    parent_path = ''
                    child_name = parts[0]
                
                # ç²å–çˆ¶æ¨¡çµ„çš„å¼•ç”¨
                if parent_path:
                    try:
                        # å˜—è©¦ä½¿ç”¨ timm/PyTorch çš„ get_submodule
                        parent_module = self.model.get_submodule(parent_path)
                    except AttributeError:
                        # å¦‚æœ get_submodule ä¸å­˜åœ¨ï¼Œä½¿ç”¨ Python æ¨™æº–æ–¹æ³•:
                        parent_module = self.model
                        for part in parent_path.split('.'):
                            parent_module = getattr(parent_module, part)
                else:
                    parent_module = self.model

                # é€²è¡Œçµæ§‹æ›¿æ›, ä½¿ç”¨ OrderedDict æ§‹é€  nn.Sequential ğŸŒŸ
                new_sequence = nn.Sequential(OrderedDict([
                    ('original_ese', module),           # åŸæœ‰çš„ ESE (nn.Module)
                    ('spatial_attn', sa_instance)       # æ–°å¢çš„ SA (nn.Module)
                ]))

                # åœ¨çˆ¶æ¨¡çµ„ä¸ŠåŸ·è¡Œæ›¿æ› (parent_module.child_name = new_sequence)
                setattr(parent_module, child_name, new_sequence)
                
                print(f"âœ… å·²å°‡ {name} æ›¿æ›ç‚º Sequential(ESE + SA)")
                sa_index += 1

        if sa_index == 0:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½• EffectiveSEModuleï¼Œè«‹ç¢ºèªæ¨¡å‹çµæ§‹æ˜¯å¦ç¬¦åˆã€‚")


    # -------------------------------------------------
    def forward(self, x):
        """æ•´é«”å‰å‘å‚³é: backbone (å« SA)ã€GAPã€FC"""
        x = self.model.forward_features(x)
        # é€™è£¡çš„ x å·²ç¶“æ˜¯ 1760 ç¶­çš„å‘é‡
        x = self.model.head(x) 
        out = self.fc(x)
        return x, out

    # -------------------------------------------------
    # --- åƒæ•¸åˆ†çµ„ (å·²ä¿®æ”¹ç‚ºåŒ¹é…æ–°çš„çµæ§‹) ---
    def get_sa_parameters(self):
        """è¿”å›æ‰€æœ‰æ³¨å…¥çš„ SA æ¨¡çµ„åƒæ•¸ (é€šéçµæ§‹åç¨±æŸ¥æ‰¾)"""
        sa_params = []
        for name, module in self.model.named_modules():
            # æˆ‘å€‘å°‹æ‰¾åœ¨çµæ§‹æ›¿æ›ä¸­å‘½åç‚º 'spatial_attn' çš„å­æ¨¡çµ„
            if isinstance(module, SpatialAttentionModule) and name.endswith('.spatial_attn'):
                sa_params.extend(list(module.parameters()))
        return iter(sa_params)

    def get_head_parameters(self):
        """è¿”å›åˆ†é¡é ­åƒæ•¸"""
        return self.fc.parameters()

    def get_backbone_parameters(self):
        """è¿”å›éª¨å¹¹åƒæ•¸(æ’é™¤ SA èˆ‡ Head)"""
        # ä½¿ç”¨é›†åˆæ“ä½œä¾†æ’é™¤æ˜¯æ›´ç©©å¥çš„æ–¹æ³•
        all_params = set(self.model.parameters())
        sa_params = set(list(self.get_sa_parameters()))
        head_params = set(list(self.get_head_parameters()))
        
        # ç”±æ–¼ self.model.head(x) åŒ…å«äº† pooling/normï¼Œå…¶åƒæ•¸å·²ç¶“åŒ…å«åœ¨ self.model.parameters() ä¸­ã€‚
        # æˆ‘å€‘å‡è¨­ head çš„åƒæ•¸éƒ½æ˜¯é è¨“ç·´æ¬Šé‡ï¼Œæ‡‰åŒ…å«åœ¨ backbone_params ä¸­ã€‚
        # é€™è£¡ä¿æŒèˆ‡åŸå§‹é‚è¼¯ä¸€è‡´ï¼Œåƒ…æ’é™¤ SA åƒæ•¸ï¼š
        backbone_params = all_params - sa_params
        
        # æ³¨æ„ï¼šå¦‚æœ self.model.head å…§éƒ¨æœ‰å¯è¨“ç·´çš„åƒæ•¸ï¼Œå®ƒå€‘æœƒè¢«åŒ…å«åœ¨ backbone_params ä¸­ï¼Œ
        # ä¸¦æ ¹æ“š freeze_and_unfreeze_params çš„é‚è¼¯é€²è¡Œè™•ç†ã€‚
        
        return iter(backbone_params)

    # ------------------------------------------------
    def update_training_stage(self, stage=1):
        """
        æ ¹æ“šè¨“ç·´éšæ®µèª¿æ•´å‡çµç­–ç•¥ã€‚
        Stage 1: å‡çµæ‰€æœ‰ Backbone, åªè¨“ç·´ SA å’Œ FCã€‚
        Stage 2: ä¿æŒ SA/FC å¯è¨“ç·´ï¼Œä¸¦è§£å‡ç¬¬ä¸€å±¤ ESE ä¹‹å¾Œçš„ä¸»å¹¹ç¶²è·¯ã€‚
        """
        if stage == 1:
            print(f"\n[Model] åˆ‡æ›è‡³ Stage 1: é–å®š Backbone, åªè¨“ç·´ SA å’Œ FC")
            # 1. å‡çµä¸»å¹¹ç¶²è·¯çš„æ‰€æœ‰åƒæ•¸
            for param in self.model.parameters():
                param.requires_grad = False
            
            # 2. è§£å‡ SA å’Œ FC (æ°¸é éœ€è¦è¨“ç·´)
            for param in self.fc.parameters():
                param.requires_grad = True
            for param in self.get_sa_parameters():
                param.requires_grad = True
                
        elif stage == 2:
            print(f"\n[Model] åˆ‡æ›è‡³ Stage 2: è§£å‡éƒ¨åˆ† Backbone é€²è¡Œå¾®èª¿")
            # é€™è£¡ä¸éœ€é‡æ–°é–å®šï¼Œç›´æ¥åŸºæ–¼ç›®å‰ç‹€æ…‹å»è§£å‡ç‰¹å®šå±¤
            found_first_ese = False
            # éæ­·ä¸»å¹¹ç¶²è·¯æ¨¡çµ„ï¼Œæ‰¾åˆ°ç¬¬ä¸€å€‹ ESEModule ä¸¦é–‹å§‹è§£å‡å…¶å¾Œçš„åƒæ•¸
            for name, module in self.model.named_modules():
                # è­˜åˆ¥ç¬¬ä¸€å€‹ ESEModule (å³çµæ§‹æ›¿æ›å¾Œçš„é‚£å€‹)
                if not found_first_ese and isinstance(module, EffectiveSEModule):
                    found_first_ese = True 
                    # è§£å‡
                    for param in module.parameters():
                        param.requires_grad = True
                # è§£å‡ç¬¬ä¸€å€‹ ESEModule ä¹‹å¾Œçš„æ‰€æœ‰å±¤
                elif found_first_ese:
                    for param in module.parameters():
                        param.requires_grad = True
            
            print("- Stage 2 è¨­å®šå®Œæˆ: SA, FC åŠç¬¬ä¸€å±¤ ESE å¾Œçš„åƒæ•¸å·²è§£å‡ã€‚")

class NormedLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super(NormedLinear, self).__init__()
        # æ¬Šé‡å½¢ç‹€æ”¹ç‚º (out_features, in_features) ç¬¦åˆ PyTorch Linear æ¨™æº–
        # é€™æ¨£ F.linear è‡ªå‹•è½‰ç½®å¾Œæ‰æœƒè®Šæˆ (in, out)ï¼Œæ‰èƒ½è·Ÿè¼¸å…¥ç›¸ä¹˜
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        # åˆå§‹åŒ–æ¬Šé‡
        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)

    def forward(self, x):
        # å°è¼¸å…¥ç‰¹å¾µ x åšæ­¸ä¸€åŒ– (æ²¿è‘— feature ç¶­åº¦)
        out = F.normalize(x, dim=1) 
        # å°æ¬Šé‡åšæ­¸ä¸€åŒ–æ™‚ï¼Œå› ç‚ºå½¢ç‹€è®Šäº†ï¼Œç¾åœ¨è¦æ²¿è‘— dim=1 (feature ç¶­åº¦) æ­¸ä¸€åŒ–
        # é€™æ¨£æ¯å€‹é¡åˆ¥çš„æ¬Šé‡å‘é‡é•·åº¦éƒ½æœƒæ˜¯ 1
        normed_weight = F.normalize(self.weight, dim=1)
        # 3. è¨ˆç®— Cosine Similarity (ä¹Ÿå°±æ˜¯æ­¸ä¸€åŒ–å¾Œçš„ Linear)
        out = F.linear(out, normed_weight)
        return out


# è‡ªå®šç¾©åµéŒ¯æ¨¡çµ„
class DebugShape(nn.Module):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def forward(self, x):
        print(f"[{self.name}] Input Shape: {x.shape}")
        return x
