import torch
from torch.utils.data import DataLoader as DL
from tqdm import tqdm
import os
import datasets
import config
import utils
from models import rdnet_tiny, rdnet_small, rdnet_large, rdnet_base
from models import RDNet_Tiny, RDNet_Small, RDNet_Base, RDNet_Large, RDNet_Base_ComplexHead, RDNet_Base_SAttention
# åŠ é€Ÿ
from accelerate import Accelerator
import timm

from Loss import LDAMLoss, FocalLoss
from models import RDNet_Base_SAttention, SpatialAttentionModule

def _GetModel(args, device):
    if args.modelName == "rdnet_small.nv_in1k":
        print("Use [rdnet_small.nv_in1k]")
        model = RDNet_Small(num_classes=args.classes).to(device)
    elif args.modelName == "rdnet_tiny.nv_in1k":
        print("Use [rdnet_tiny.nv_in1k]")
        model = RDNet_Tiny(num_classes=args.classes).to(device)
    elif args.modelName == "rdnet_base.nv_in1k":
        print("Use [rdnet_base.nv_in1k]")
        model = RDNet_Base(num_classes=args.classes).to(device)
    elif args.modelName == "rdnet_large.nv_in1k":
        print("Use [rdnet_large.nv_in1k]")
        model = RDNet_Large(num_classes=args.classes).to(device)
    elif args.modelName == 'rdnet_base_reload_head':
        print("Use [rdnet_base & reload_head]")
        model = RDNet_Base_ComplexHead(num_classes=args.classes)
    elif args.modelName == 'rdnet_base_SAttention':
        print("Use [rdnet_base & spatial attention]")
        model = RDNet_Base_SAttention(num_classes=args.classes, sa_kernel_size=3)
    
    # model = timm.create_model(
    #     args.modelName, 
    #     pretrained=True, 
    #     in_chans=3, 
    #     num_classes=args.classes,
    # ).to(args.device)
    
    return model


def load_best_model(args, model):
    """
    è¼‰å…¥è¨“ç·´å¥½çš„æœ€ä½³æ¨¡å‹ checkpoint
    """
    ckpt_path = './checkpoint/rdnet_base_SAttention_bz16__v4/rdnet_base_SAttention_ckpt_epoch24.pth.tar'
    print(f"âœ… è¼‰å…¥æ¨¡å‹æ¬Šé‡ï¼š{ckpt_path}")
    checkpoint = torch.load(ckpt_path, map_location=args.device)
    model.load_state_dict(checkpoint['model_state_dict'])
    args = checkpoint['args']
    return model, args

def main(args):
    print("===== Test start =====")
    
    #å»ºç«‹åŠ é€Ÿå™¨
    accelerator = Accelerator()
    device = accelerator.device

    # === è¼‰å…¥æ¨¡å‹ ===
    model = _GetModel(args, device)
    model, args = load_best_model(args, model)

    # === è¼‰å…¥ Test Dataset ===
    test_dataset = datasets.ImagesDataset(args=args, phase='test')
    print("Test dataset:", len(test_dataset))

    test_dataloader = DL(
        test_dataset,
        batch_size=args.batch_size,
        num_workers=args.workers,
        shuffle=False
    )


    criterion = FocalLoss(
        gamma=2.0, # Gamma è¶Šå¤§ï¼Œå°å¤šæ•¸é¡åˆ¥çš„æŠ‘åˆ¶è¶Šå¼·ï¼Œå¼·åˆ¶é—œæ³¨å°‘æ•¸é¡åˆ¥
        alpha=None,             # å…ˆä¸ä½¿ç”¨ alphaï¼Œè®“ gamma å°ˆæ³¨æ–¼é›£æ˜“æ¨£æœ¬åˆ†é¡
        reduction='mean', 
        task_type='multi-class',
        num_classes=args.classes
    )


    model, criterion = accelerator.prepare(model, criterion)
    print(model)

    # === åŸ·è¡Œ SA è¦–è¦ºåŒ– (åƒ…æŠ“å–ç¬¬ä¸€å€‹ Batch çš„ç¬¬ä¸€å¼µå½±åƒ) ===
    if args.modelName == 'rdnet_base_SAttention':
        VISUALIZATION_OUTPUT_DIR = "./sa_attention_maps"
        # é‡æ–°åˆå§‹åŒ– Dataloader ä»¥ç¢ºä¿æ‹¿åˆ°æ–°çš„è¿­ä»£å™¨ (æˆ–ç¢ºä¿åœ¨ test_one_epoch å¾Œ Dataloader ä»å¯ç”¨)
        test_dataloader_viz = DL(test_dataset, batch_size=args.batch_size, num_workers=args.workers, shuffle=False)
        
        visualize_single_image_attention(args, model, test_dataloader_viz, VISUALIZATION_OUTPUT_DIR, device)


    print("===== Test end =====")


def visualize_single_image_attention(args, model, dataloader, output_dir, device):
    """
    å¾ dataloader æå–ç¬¬ä¸€å€‹ Batch çš„æ‰€æœ‰å½±åƒï¼Œç‚ºæ¯å¼µå½±åƒå‰µå»ºç¨ç«‹è³‡æ–™å¤¾ï¼Œ
    ä¸¦å°‡æ³¨æ„åŠ›åœ–ç–ŠåŠ åˆ°åŸå§‹åœ–åƒä¸Šå„²å­˜ã€‚
    """
    # ğŸŒŸ ä¿®æ­£é» 1: è§£åŒ…æ¨¡å‹ ğŸŒŸ
    # model.module ç”¨æ–¼ DistributedDataParallel/DataParallelï¼Œå¦å‰‡ä½¿ç”¨åŸæ¨¡å‹
    model_unwrapped = model.module if hasattr(model, 'module') else model

    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦åŒ…å« SA æ¨¡çµ„
    if 'SAttention' not in model_unwrapped.__class__.__name__:
        print("âš ï¸ æ¨¡å‹ä¸åŒ…å« SAttention æ¨¡çµ„ï¼Œè·³éè¦–è¦ºåŒ–ã€‚")
        return

    model.eval()
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ç²å–æ‰€æœ‰ SpatialAttentionModule å¯¦ä¾‹
    sa_modules = []
    # ğŸŒŸ ä¿®æ­£é» 2: ä½¿ç”¨è§£åŒ…å¾Œçš„æ¨¡å‹éæ­·æ¨¡çµ„ ğŸŒŸ
    for name, module in model_unwrapped.named_modules():
        if isinstance(module, SpatialAttentionModule) and name.endswith('.spatial_attn'):
            sa_modules.append((name, module))
            
    if not sa_modules:
        print("æ¨¡å‹ä¸­æœªæ‰¾åˆ° SpatialAttentionModuleã€‚")
        return

    print(f"\næ‰¾åˆ° {len(sa_modules)} å€‹ SA æ¨¡çµ„é€²è¡Œè¦–è¦ºåŒ–ã€‚")
    
    # 2. åªå¾ dataloader ä¸­å–å‡ºç¬¬ä¸€å€‹ Batch
    try:
        first_batch = next(iter(dataloader)) 
        images, labels = first_batch
    except StopIteration:
        print("Dataloader ç‚ºç©ºã€‚")
        return
        
    # 3. åŸ·è¡Œæ¨¡å‹å‰å‘å‚³é
    images_gpu = images.to(device)
    
    with torch.no_grad():
        # é‹è¡Œæ¨¡å‹ï¼Œè®“ SA æ¨¡çµ„å…§å„²å­˜æœ€æ–°çš„ attn_map
        model(images_gpu) 

    # 4. è¿­ä»£æ•´å€‹ Batch ä¸­çš„æ¯å¼µå½±åƒï¼Œä¸¦å„²å­˜çµæœ
    batch_size = images.shape[0]
    total_saved_images = 0
    
    for batch_idx in range(batch_size):
        
        # A. å‰µå»ºæ¯å€‹å½±åƒçš„ç¨ç«‹è¼¸å‡ºè³‡æ–™å¤¾
        image_output_dir = os.path.join(output_dir, f"image_{batch_idx:03d}")
        os.makedirs(image_output_dir, exist_ok=True)
        
        # B. å°ç•¶å‰å½±åƒé€²è¡Œåæ­£è¦åŒ– (ä½¿ç”¨ CPU ä¸Šçš„ images)
        single_image_denorm = utils.denormalize_image(images[batch_idx], args.mean, args.std) 
        
        # C. è¿­ä»£æ‰€æœ‰ SA æ¨¡çµ„ä¸¦å„²å­˜çµæœ
        for i, (name, sa_module) in enumerate(sa_modules):
            
            # å¾è§£åŒ…å¾Œçš„ SA å¯¦ä¾‹è¨ªå•å„²å­˜çš„æ³¨æ„åŠ›åœ–
            attn_map = sa_module.latest_attn_map
            if attn_map is None:
                continue
            
            # æå–ç•¶å‰ Batch ç´¢å¼• (batch_idx) çš„ Attention Map (H, W)
            single_map_np = attn_map[batch_idx, 0, :, :].cpu().numpy()
            
            # ç²å– blocks.X
            layer_name = name.rsplit('.', 2)[-2]
            
            # å„²å­˜ç–ŠåŠ å¾Œçš„åœ–åƒåˆ°è©²å½±åƒå°ˆå±¬è³‡æ–™å¤¾
            file_name_overlay = f"SA_Overlay_Layer{i}_{layer_name}.png"
            overlay_output_path = os.path.join(image_output_dir, file_name_overlay)
            
            # èª¿ç”¨ç–ŠåŠ å‡½å¼
            utils.overlay_attention_on_image(
                original_img_tensor=single_image_denorm, 
                attn_map_np=single_map_np,
                output_path=overlay_output_path,
                layer_name=layer_name
            )
            total_saved_images += 1
            
    print(f"\nâœ¨ ç©ºé–“æ³¨æ„åŠ›ç–ŠåŠ åœ–åƒå·²å„²å­˜è‡³ï¼š{output_dir} (å…± {batch_size} å€‹å½±åƒè³‡æ–™å¤¾ï¼Œ{total_saved_images} å¼µåœ–ç‰‡)")


if __name__ == '__main__':
    main(config.GetArgument())
