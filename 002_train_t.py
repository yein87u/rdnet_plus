import datasets
import config
from sklearn.model_selection import StratifiedShuffleSplit
import torch
from torch.utils.data import DataLoader as DL
import timm
import utils
from tqdm import tqdm
import os
from models import rdnet_tiny, rdnet_small, rdnet_large, rdnet_base
from models import RDNet_Tiny, RDNet_Small, RDNet_Base, RDNet_Large, RDNet_Base_SAttention
import numpy as np

from typing import Dict, Any
import torch.nn as nn

from Loss import LDAMLoss, FocalLoss
from torch.utils.data import WeightedRandomSampler

def _GetModel(args):
    if args.modelName == "rdnet_small.nv_in1k":
        print("Use [rdnet_small.nv_in1k]")
        model = RDNet_Small(num_classes=args.classes)
    elif args.modelName == "rdnet_tiny.nv_in1k":
        print("Use [rdnet_tiny.nv_in1k]")
        model = RDNet_Tiny(num_classes=args.classes)
    elif args.modelName == "rdnet_base.nv_in1k":
        print("Use [rdnet_base.nv_in1k]")
        model = RDNet_Base(num_classes=args.classes)
    elif args.modelName == "rdnet_large.nv_in1k":
        print("Use [rdnet_large.nv_in1k]")
        model = RDNet_Large(num_classes=args.classes)
    elif args.modelName == 'rdnet_base_SAttention':
        print("Use [rdnet_base & spatial attention]")
        model = RDNet_Base_SAttention(num_classes=args.classes, sa_kernel_size=3, drop_rate=0.2)
    
    # model = timm.create_model(
    #     args.modelName, 
    #     pretrained=True, 
    #     in_chans=3, 
    #     num_classes=args.classes,
    # ).to(args.device)

    return model

def _GetOptimizer(args, model: nn.Module):
    if '.nv_in1k' in args.modelName:
        param_groups = model.parameters()
    else:
        optimizer = None
        # å®šç¾©å­¸ç¿’çŽ‡ä¹˜æ•¸, æ–°æ¨¡çµ„ (SAå’ŒFC) ç›¸å°æ–¼å¾®èª¿å±¤çš„å­¸ç¿’çŽ‡ä¹˜æ•¸, 5 or 10
        LR_MULTIPLIER = 2.0

        # SA æ¨¡çµ„åƒæ•¸
        sa_params = model.get_sa_parameters()
        # FC å±¤åƒæ•¸
        fc_params = model.fc.parameters()

        # ç²å–æ‰€æœ‰å¯è¨“ç·´åƒæ•¸çš„é›†åˆ
        all_trainable_params = [p for p in model.parameters() if p.requires_grad]
        # å°‡ SA å’Œ FC åƒæ•¸è½‰æ›ç‚ºé›†åˆä»¥ä¾¿æ–¼æŽ’é™¤
        sa_fc_set = set(list(model.fc.parameters()) + list(model.get_sa_parameters()))
        # ä¸»å¹¹ç¶²è·¯å¾®èª¿åƒæ•¸ = æ‰€æœ‰å¯è¨“ç·´åƒæ•¸ - SAåƒæ•¸ - FCåƒæ•¸
        backbone_params = [p for p in all_trainable_params if p not in sa_fc_set]
        
        # å‰µå»ºåƒæ•¸å­—å…¸åˆ—è¡¨
        param_groups = [
            # Group 1: ä¸»å¹¹ç¶²è·¯å¾®èª¿ (é è¨“ç·´æ¬Šé‡) - ä½¿ç”¨åŸºç¤ŽLR
            {'params': backbone_params, 'lr': args.lr, 'name': 'backbone_finetune'},
            # Group 2: FC å±¤å’Œ SA æ¨¡çµ„ (éš¨æ©Ÿåˆå§‹åŒ–) - ä½¿ç”¨è¼ƒé«˜LR
            {'params': list(sa_params) + list(fc_params), 'lr': args.lr * LR_MULTIPLIER, 'name': 'new_modules'},
        ]

    # --- 4. åˆå§‹åŒ–å„ªåŒ–å™¨ ---
    if(args.optimizer == "adamw"):
        optimizer = torch.optim.AdamW(
            param_groups,  # å‚³å…¥åƒæ•¸å­—å…¸åˆ—è¡¨
            lr=args.lr,  # é€™è£¡è¨­ç½®çš„lræœƒè¢«param_groupsä¸­çš„lrè¦†è“‹
            weight_decay=args.weight_decay
        )
    
    return optimizer

def _GetScheduler(args, optimizer):
    scheduler = None
    if(args.scheduler == "CosineAnnealingWarmRestarts"):
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, 
            T_0=5, 
            T_mult=1, 
            eta_min=1e-6
        )
    if(args.scheduler == "CosineAnnealingLR"):
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=50,  # ç¸½è¨“ç·´ Epoch æ•¸
            eta_min=2e-5
        )
    
    return scheduler


def one_epoch(args, model, dataloader, optimizer, criterion):
    if args.phase == "train":
        model.train()
    elif args.phase == "val":
        model.eval()
    
    total_loss, correct, total = 0, 0, 0
    all_preds, all_labels = [], []

    with tqdm(dataloader, desc=f"{args.phase}", ncols=100, leave=False) as pbar:
        for images, labels in pbar:
            # è‹¥ç‚ºtrainéšŽæ®µ, æ‰“é–‹æ¢¯åº¦è¿½è¹¤
            with torch.set_grad_enabled(args.phase == "train"):
                images, labels = images.to(args.device), labels.to(args.device)
                optimizer.zero_grad()
                if '.nv_in1k'  in args.modelName or 'SAttention' in args.modelName:
                    features, logits = model(images)   #output.shape = (batch pred_result)
                else:
                    logits = model(images)   #output.shape = (batch pred_result)
                
                loss = criterion(logits, labels)

                if args.phase == "train":
                    loss.backward()
                    optimizer.step()

                total_loss += loss.item() * images.size(0)
                _, preds = torch.max(logits, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)
                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
            
            pbar.set_postfix({
                    "Acc": f"{(correct/total*100):.2f}%",
                    "Loss": f"{(total_loss/total):.4f}"
                })

        epoch_loss = total_loss / total
        acc, f1, precision, recall = utils.evaluate(all_labels, all_preds)

    return acc, f1, precision, recall, epoch_loss


def _SaveModel(args, model):
    # å»ºç«‹è³‡æ–™å¤¾ï¼Œä¾‹å¦‚ rdnet_small_fold1_v1_bz16
    folder_name = f"{args.modelName}_bz{args.batch_size}"
    path = os.path.join(args.root_model, folder_name + "_" + args.train_version)
    if not os.path.isdir(path):
        os.makedirs(path)

    # å„²å­˜ checkpoint
    checkpoint_path = os.path.join(path, f"{args.modelName}_ckpt.pth.tar")
    utils.save_checkpoint(
        checkpoint_path,
        {
            'args': args,
            'Accuracy': args.best_acc,
            'Loss': args.best_loss,
            'F1-Score': args.best_f1,
            'Precision': args.best_precision,
            'Recall': args.best_recall,
            'model_state_dict': model.state_dict(),
        },
        False
    )

    print(" | ~~New Best Model Find!~~")

def main(args):
    print("===== main start =====")

    train_dataset = datasets.ImagesDataset(args=args, phase='train')
    print(f'mean: {args.mean}\nstd: {args.std}')
    print("Train dataset:", len(train_dataset))

    # === è®€å– Val Dataset ===
    val_dataset = datasets.ImagesDataset(args=args, phase='val')

    train_dataloader = DL(train_dataset,
                    batch_size=args.batch_size,
                    num_workers=args.workers,
                    shuffle=True, 
                    drop_last=True)

    val_dataloader = DL(val_dataset,
                    batch_size=args.batch_size,
                    num_workers=args.workers, 
                    shuffle=False)
    
    # åˆå§‹åŒ–æ¯å€‹ fold çš„ metricsã€modelã€optimizerã€scheduler
    args = config.MetricsInit(args)
    model = _GetModel(args)

    base_lr = args.lr
    if hasattr(model, 'update_training_stage'):
        model.update_training_stage(stage=1)

    # print(model)
    # # è¨ˆç®—æ‰€æœ‰åƒæ•¸æ•¸é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    optimizer = _GetOptimizer(args, model)
    scheduler = _GetScheduler(args, optimizer)

    cls_num_list = train_dataset.cls_num_list
    criterion = nn.CrossEntropyLoss()
    # å‚™ç”¨ Lossï¼š LDAM å¸¶æœ‰é‚Šç•Œèª¿æ•´
    ldam_criterion = LDAMLoss(cls_num_list=cls_num_list, max_m=0.8, s=30) # max_m=0.5 å’Œ s=30 æ˜¯å¸¸è¦‹çš„èµ·å§‹å€¼

    # åˆå§‹ Loss å‡½æ•¸è¨­å®šç‚ºåŸºæº– Loss
    ldam_criterion = ldam_criterion.to(args.device)

    model, optimizer, train_dataloader, val_dataloader, \
    criterion, scheduler = args.accelerator.prepare(
        model, optimizer, train_dataloader, val_dataloader,
        criterion, scheduler
    )    
    
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': [],
    }
    
    for epoch in range(args.epochs):
        args.epoch = epoch + 1

        if args.epoch == args.unfreeze_epoch and hasattr(model, 'update_training_stage'):
            print(f"\nðŸŒŸ Epoch {args.epoch}: è§¸ç™¼è§£å‡é‚è¼¯ï¼é‡æ–°åˆå§‹åŒ–å„ªåŒ–å™¨...")
            # è§£å‡æ¨¡åž‹å±¤
            model.update_training_stage(stage=2)
            
            # èª¿æ•´å­¸ç¿’çŽ‡ (å¾®èª¿éšŽæ®µé€šå¸¸ç”¨æ›´å°çš„ LR)
            args.lr = base_lr * 0.5
            # é‡æ–°å»ºç«‹å„ªåŒ–å™¨, å› ç‚ºåƒæ•¸çš„ requires_grad è®Šäº†ï¼ŒèˆŠçš„ optimizer ä¸æœƒæ›´æ–°æ–°è§£å‡çš„å±¤
            optimizer = _GetOptimizer(args, model)
            # é‡æ–°å»ºç«‹ Scheduler, å› ç‚º optimizer æ›äº†
            scheduler = _GetScheduler(args, optimizer)
            # Acceleratorå¿…é ˆé‡æ–° prepare æ–°çš„ optimizer
            if hasattr(args, 'accelerator'):
                optimizer, scheduler = args.accelerator.prepare(optimizer, scheduler)
                print("âœ¨ Accelerator: Optimizer re-prepared.")

        if args.epoch == args.drw_start_epoch:
            print(f"\nâš¡ Epoch {args.epoch}: å•Ÿç”¨ LDAM Loss (Deferred Re-Weighting)")
            # å°‡è¨“ç·´ä½¿ç”¨çš„ Loss å‡½æ•¸åˆ‡æ›ç‚ºå¸¶æœ‰é‚Šç•Œèª¿æ•´çš„ LDAM Loss
            criterion = ldam_criterion.to(args.device)
        
        # === Training ===
        args.phase = "train"
        train_acc, train_f1, train_precision, \
        train_recall, train_loss = one_epoch(args, 
                                            model, 
                                            train_dataloader, 
                                            optimizer, 
                                            criterion)
        
        # === Validation ===
        args.phase = "val"
        val_acc, val_f1, val_precision, \
        val_recall, val_loss = one_epoch(args, 
                                        model, 
                                        val_dataloader, 
                                        optimizer, 
                                        criterion)
        
        scheduler.step()
        args.lr = optimizer.param_groups[0]['lr']

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)

        print(f"Epoch{args.epoch} T_Acc: {train_acc:.2f}% | V_Acc: {val_acc:.2f} | T_Loss: {train_loss:.4f} | V_Loss: {val_loss:.4f} | T_F1: {train_f1:.4f} | V_F1: {val_f1:.4f} | LR: {args.lr:.2e}", end="")
        
        # å„²å­˜ best model
        if val_acc > args.best_acc:
            args.best_acc = val_acc
            args.best_loss = val_loss
            args.best_f1 = val_f1
            args.best_recall = val_recall
            args.best_precision = val_precision
            _SaveModel(args, model)
        else:
            print()
    
    utils.plot_history(history, args)

    print(f"!~~~~~~~~~~~~~~~!\nBest Acc = {args.best_acc:.2f}%\nBest Loss = {args.best_loss:.4f}\n!~~~~~~~~~~~~~~~!\n")
    print("===== main end =====")

if __name__ == '__main__':
    main(config.GetArgument())
